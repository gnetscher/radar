## Training Models

Run the following command with the options outlined in main.py to train the classification models.
The script allows you to train models with only image data, only radar data, or both data combined using late fusion.

```
ipython main.py --radarDir <path-to-radar-files> --expType joint --gpu 1
```
The options to pass to this script are as follows, more info can be found in the parse_args() function of main.py:

- --radarDir = where radar .mat files are stored
- --logFile = log file in which to store caffe training output. Helpful for monitoring learning
- --expType = either 'joint', 'image', 'radar'. 'joint' uses both image and radar data. 'image'
		and 'radar' use their own respective data sources
- --outputDir = where to store net prototxt files, solver files, and other misc files generated by caffe
- --weights = pretrained weights file. We start with a CaffeNet pretrained on ImageNet
- --snapshotDir = where to store solver snapshots during training
- --batchSize = batch size for training
- --gpu = which gpu id on the machine you wish to run the experiment


## Testing

Run the following command after filling in the appropriate arguments. More details can be found in testNet.py

```
ipython testNet.py --netFile <net-prototxt> --weights <trained-weight-file> --outFile <store-results-here>
```

## Visualize Person Detection Output

To see how our pretrained rcnn model detects people in images, run the following command:

```
ipython rcnnVis.py
```

This script will save images to a folder called vis_output, where each image contains overlaid detections and ground truths.
This will be done on all the sleeplab videos. This script also returns the mean AP for the predictions made by our pretrained model.
Our current results show a meanAP of 72.69

### Pretrained Models

The pretrained models for the different experimental configurations can be found <a href="https://drive.google.com/drive/folders/0B6C-DilqG6vIM1FTMlQ4eWktY2s?usp=sharing">Here</a>. There are models trained on image data, radar data, and both.


